# Optimization -- General Principles and Profilling Techniques

## The three rules of optimization
無論結果如何，優化都是有代價的。 當一段代碼有效時，最好（有時）不去管它，而不是不惜一切代價讓它變得更快。 在進行任何類型的優化時，需要牢記一些規則：
- 先讓它發揮作用
- 從用戶的角度出發 
- 保持代碼的可讀性

### Make it work first 
一個非常常見的錯誤是在編寫代碼時嘗試對其進行優化。 這在很大程度上是毫無意義的，因為真正的瓶頸通常位於您從未想過的地方。
應用程序通常由非常複雜的交互組成，在真正使用之前不可能全面了解正在發生的事情。
當然，這不是編寫函數或方法而不試圖使其盡可能快的理由。 您應該注意盡可能降低其複雜性並避免無用的重複。 但首要目標是讓它發揮作用。 優化工作不應阻礙這一目標。

對於行級代碼，Python 的哲學是只有一種方法，最好只有一種方法。 通常，編寫更少的代碼比編寫更多的代碼更好更快。

對於非常專業的領域，例如科學計算或遊戲，專業庫的使用和外部化可能從一開始就不可避免。 另一方面，使用像 NumPy 這樣的庫可能會簡化特定功能的開發，並最終生成更簡單、更快的代碼。 此外，如果有一個好的庫可以為您完成，則您不應該重寫一個函數。
例如，Soya 3D 是基於 OpenGL 的遊戲引擎（參見 http://home.gna.org/oomadness/en/soya3d/index.html），在渲染真實圖像時使用 C 和 Pyrex 進行快速矩陣運算

### Work from the user's point of view 
我見過一些團隊致力於優化應用程序服務器的啟動時間，這些應用程序服務器在已經啟動並運行時運行得非常好。 一旦他們完成加速，他們就將這項工作推廣給他們的客戶。 他們有點沮喪地註意到客戶並不真正關心它。 這是因為加速工作的動機不是用戶反饋，而是開發人員的觀點。 構建系統的人每天要多次啟動服務器。 因此，啟動時間對他們來說意義重大，但對他們的客戶而言則不然。

雖然從絕對的角度來看，讓程序啟動得更快是一件好事，但團隊應該謹慎地確定優化工作的優先級，並問自己以下問題：
- 有人要求我讓它更快嗎？
- 誰發現程序慢？
- 它真的很慢，還是可以接受？
- 讓它運行得更快需要多少錢，值得嗎？ 哪些部位需要快？
請記住，優化是有成本的，開發人員的觀點對客戶來說毫無意義，除非您正在編寫框架或庫，而客戶也是開發人員。

### Keep the code readable and maintainable 
即使 Python 試圖使通用代碼模式最快，優化工作也可能會混淆您的代碼並使其真正難以閱讀。 在生成可讀且因此可維護的代碼和破壞代碼以使其更快之間保持平衡。
當您已達到優化目標的 90%，而剩下的 10% 使您的代碼完全不可讀時，停止那裡的工作或尋找其他解決方案可能是個好主意。

## Optimization strategy
假設您的程序有一個您需要解決的實際速度問題。 不要試圖猜測如何讓它更快。 看代碼往往很難發現瓶頸，需要一套工具才能找到真正的問題所在。
一個好的優化策略可以從三個步驟開始：
- 尋找另一個罪魁禍首：確保第三方服務器或資源沒有故障
- 擴展硬件：確保資源充足
- 編寫速度測試：創建具有速度目標的場景

### Find another culprit
通常，性能問題發生在生產級別，客戶會提醒您它沒有像測試軟件時那樣工作。 可能會出現性能問題，因為應用程序未計劃在具有大量用戶和數據量增加的現實世界中運行。
但是如果應用程序與其他應用程序交互，首先要做的是檢查瓶頸是否位於這些交互上。 例如，數據庫服務器或 LDAP 服務器可能會產生額外的開銷，並可能使一切變慢。
還應考慮應用程序之間的物理鏈接。 也許您的應用程序服務器和 Intranet 中的另一台服務器之間的網絡鏈接由於配置錯誤或擁塞而非常慢。
設計文檔應提供所有交互的圖表和每個鏈接的性質，以全面了解系統並在嘗試解決速度問題時提供幫助。

### Scale the hardware
當沒有可用的易失性內存時，系統開始使用硬盤來存儲數據。 這是交換。
這涉及大量開銷並且性能急劇下降。 從用戶的角度來看，系統在這個階段被認為是死的。 因此，擴展硬件以防止這種情況很重要。

雖然在系統上擁有足夠的內存很重要，但確保應用程序不會瘋狂運行並佔用過多內存也很重要。 例如，如果一個程序處理可能重達數百兆字節的大視頻文件，它不應該將它們完全加載到內存中，而是處理塊或使用磁盤流。
磁盤使用也很重要。 如果 I/O 錯誤隱藏在嘗試在磁盤上重複寫入的代碼中，那麼完整的分區可能真的會減慢您的應用程序。 此外，即使代碼只嘗試寫入一次，硬件和操作系統也可能嘗試寫入多次。


請注意，擴展硬件（垂直擴展）有一些明顯的局限性。 您無法將無限數量的硬件安裝到單個機架中。 此外，高效的硬件非常昂貴（收益遞減法則），因此這種方法也有經濟上的限制。 從這個角度來看，擁有可以通過添加新的計算節點或工作人員（水平擴展）來擴展的系統總是更好的。 這使您可以使用具有最佳性能/價格比的商品軟件來擴展您的服務。

不幸的是，設計和維護高度可擴展的分佈式系統既困難又昂貴。 如果您的系統不能輕鬆地水平擴展，或者垂直擴展更快更便宜，那麼最好這樣做，而不是浪費時間和資源重新設計系統架構。 請記住，硬件總是會隨著時間的推移變得更快、更便宜。 許多產品都停留在這個最佳位置，它們的擴展需求與提高硬件性能的趨勢保持一致。

### Writing a speed test
在開始優化工作時，重要的是使用類似於測試驅動開發的工作流程，而不是連續運行一些手動測試。 一個好的做法是在編寫要優化的調用序列的應用程序中專門使用一個測試模塊。 擁有此場景將幫助您在優化應用程序時跟踪您的進度。
您甚至可以在設置一些速度目標的地方編寫一些斷言。 為了防止速度回歸，這些測試可以在代碼優化後保留：

## Finding bottlenecks
Finding bottlenecks is done by:
- Profiling CPU usage 
- Profiling memory usage 
- Profiling network usage

### Profiling CPU usage
瓶頸的第一個來源是您的代碼。 標準庫提供了執行代碼分析所需的所有工具。 它們基於確定性方法。
確定性分析器通過在最低級別添加計時器來測量每個函數花費的時間。 這會引入一些開銷，但可以很好地了解時間消耗在哪裡。 另一方面，統計分析器對指令指針的使用情況進行採樣，而不檢測代碼。 後者不太準確，但允許全速運行目標程序。
有兩種方法可以分析代碼：
- 宏分析：
  這會在整個程序被使用時對整個程序進行分析並生成統計信息
- 微分析：
  通過手動檢測程序的精確部分



#### Macro-profiling
宏分析是通過在特殊模式下運行應用程序來完成的，在這種模式下，解釋器被用來收集代碼使用情況的統計數據。 Python為此提供了幾個工具：
- profile：這是一個純Python實現
- cProfile：這是一個 C 實現，提供與配置文件工具相同的接口，但開銷較小

大多數 Python 程序員的推薦選擇是 cProfile，因為它可以減少開銷。 無論如何，如果您需要以某種方式擴展探查器，那麼 profile 可能是更好的選擇，因為它不使用 C 擴展。
這兩種工具具有相同的界面和用法，因此我們將僅使用其中一種來展示它們的工作原理。 以下是一個 myapp.py 模塊，其中包含我們將使用 cProfile 測試的主要功能：
> chapter11/myapp.py
可以直接從提示中調用該模塊，結果匯總如下：
`python3 -m cProfile myapp.py`
提供的統計數據是分析器填充的統計對象的打印視圖。 手動調用該工具可以是：
```python
import cProfile
from myapp import main 
profiler = cProfile.Profile()
profiler.runcall(main)
profiler.print_stats()
```

#### Micro-profiling
當發現慢函數時，有時需要做更多的分析工作來測試程序的一部分。 這是通過在速度測試中手動檢測部分代碼來完成的。
例如，可以從裝飾器中使用 cProfile 模塊：
```python
import tempfile,os,cProfile,pstats
def profile(column='time',list=5):
    def _profile(function):
        def __profile(args,*kw):
            s  = tempfile.mktemp()
            profiler = cProfile.Profile()
            profiler.runcall(function,args,*kw)
            profiler.dump_stats(s)
            p = pstats.Stats(s)
            p.sort_stats(column).print_stats(list)
        return __profile
    return _profile 

from myapp import main 
@profile('time',6)
def main_profiled():
    return main()
main_profiled()
```
這種方法允許測試應用程序的某些部分並提高統計輸出。 但是在這個階段，擁有一個被調用者列表可能並不有趣，因為該函數已經被指出是要優化的函數。 唯一有趣的信息是知道它有多快，然後再提高它。
timeit 更好地滿足了這一需求，它提供了一種簡單的方法來測量帶有主機系統提供的最佳底層計時器（time.time 或 time.clock）的小代碼片段的執行時間：

#### Measuring Pystones
測量執行時間時，結果取決於計算機硬件。 為了能夠產生一個通用的衡量標準，最簡單的方法是對固定代碼序列的速度進行基準測試，並從中計算出一個比率。 從那裡，
  
函數所花費的時間可以轉換為可以在任何計算機上進行比較的通用值。
### Profiling memory usage
優化應用程序時可能遇到的另一個問題是內存消耗。 如果一個程序開始消耗太多內存以致於系統開始交換，那麼您的應用程序中可能有一個地方創建了太多對象，或者您不打算保留的對象仍然通過一些意外引用保持活動狀態。 這通常很容易通過經典分析來檢測，因為消耗足夠的內存來進行系統交換涉及大量可以檢測到的 CPU 工作。 但有時它並不明顯，必須分析內存使用情況。

#### How python deals with memory
當您使用 CPython 實現時，內存使用情況可能是 Python 中最難分析的事情。 雖然 C 等語言允許您獲取任何元素的內存大小，但 Python 永遠不會讓您知道給定對象消耗了多少內存。 這是由於語言的動態特性，以及語言用戶不能直接訪問內存管理這一事實

內存管理的一些原始細節已經在第 7 章“其他語言的 Python 擴展”中進行了解釋。 我們已經知道 CPython 使用引用計數來管理對象分配。 這是確定性算法，可確保在對象的引用計數變為零時觸發對象釋放。 儘管是確定性的，但這個過程不容易手動跟踪，也不容易在復雜的代碼庫中進行推理。 此外，在引用計數級別上釋放對象並不一定意味著解釋器釋放了實際的進程堆內存。 根據 CPython 解釋器編譯標誌、系統環境或運行時上下文，內部內存管理層可能決定保留一些空閒內存塊以供將來重新分配，而不是完全釋放它。

CPython 實現中的其他微優化也使得預測實際內存使用量變得更加困難。 例如，指向同一個短字符串或小整數值的兩個變量可能指向也可能不指向內存中的同一個對象實例。
儘管相當可怕且看似複雜，但 Python 中的內存管理有很好的文檔記錄（請參閱 https://docs.python.org/3/c-api/memory.html）。 請注意，前面提到的微優化可以在大多數情況下
   
例，在調試內存問題時被忽略。 此外，引用計數大致基於一個簡單的語句——如果一個給定的對像不再被引用，它就會被刪除。 換句話說，函數中的所有局部引用都在解釋器之後被刪除：
- 離開功能
- 確保不再使用該對象
因此，保留在內存中的對像是：
- 全局對象
- 仍以某種方式引用的對象


Python 中的引用計數很方便，使您免於手動跟踪對象的對象引用的義務，因此您不必手動銷毀它們。 儘管這引入了另一個問題，因為開發人員從不清理內存中的實例，如果開發人員不注意他們使用數據結構的方式，它可能會以不受控制的方式增長。
通常的內存吞噬者是：
- 不受控制地增長的緩存
- 全局註冊實例但不跟踪其使用情況的對象工廠，例如每次調用查詢時即時使用的數據庫連接器創建器
- 未正確完成的線程
- 具有 __del__ 方法並參與循環的對像也是內存吞噬者。 在舊版本的 Python 中（3.4 之前的版本），垃圾收集器不會打斷循環，因為它無法確定應該先刪除哪個對象。 因此，您將洩漏內存。 在大多數情況下，使用此方法不是一個好主意。

不幸的是，引用計數的管理必須在 C 擴展中使用帶有 Py_INCREF() 和 Py_DECREF() 宏的 Python/C API 手動完成。 我們在前面的第 7 章“其他語言中的 Python 擴展”中討論了處理引用計數和引用所有權的注意事項，因此您應該已經知道這是一個充滿各種陷阱的相當困難的話題。 這就是為什麼大多數內存問題都是由未正確編寫的 C 擴展引起的。

#### Profiling memory
在開始查找 Python 中的內存問題之前，您應該知道 Python 中內存洩漏的性質非常特殊。 在某些編譯型語言（如 C 和 C++）中，內存洩漏幾乎完全是由分配的內存塊引起的，這些內存塊不再被任何指針引用。

如果沒有引用內存，就不能釋放它，這種情況稱為內存洩漏。 在 Python 中，沒有可供用戶使用的低級內存管理，因此我們寧願處理洩漏引用——對不再需要但未被刪除的對象的引用。 這停止了解釋器釋放資源，但與 C 中的內存洩漏情況不同。當然，總是有 C 擴展的例外情況，但它們是一種不同的野獸，需要完全不同的工具鏈，並且不容易從 Python 代碼。

因此，**Python 中的內存問題主要是由意外或計劃外的資源獲取模式引起的。** 這種情況很少發生，這是由於內存分配和釋放例程處理不當而導致的真實錯誤的影響。 在使用 Python/C API 編寫 C 擴展時，此類例程僅適用於 CPython 中的開發人員，您將很少使用它們，如果有的話。 因此，Python 中大多數所謂的內存洩漏主要是由軟件過於復雜以及組件之間難以跟踪的微小交互引起的。 為了發現和定位軟件的此類缺陷，您需要了解程序中的實際內存使用情況。

幸運的是，有一些工具可用於製作內存快照併計算加載對象的數量和大小。 但是請記住，Python 不會輕易釋放內存，它更願意保留內存以備再次需要時使用。

##### objgraph 

#### C code memory leaks

### Profiling network usage
